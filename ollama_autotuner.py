import ollama
import time
import pandas as pd
import itertools
import multiprocessing
import sys
import os
import webbrowser

from queue import Empty
from datetime import datetime
from jinja2 import Environment, FileSystemLoader
from evaluation_dataset import HALLUCINATION_EVAL_SET, LONG_CONTEXT_PERFORMANCE_PROMPT
from utils import get_model_size_in_billions, get_local_ollama_models


def ollama_generate_worker(queue, model_name, prompt, options):
    try:
        if options.get('stream', False):
            start_time = time.time()
            stream = ollama.generate(model=model_name, prompt=prompt, options=options, stream=True)
            first_token_time = None; token_count = 0
            for chunk in stream:
                if first_token_time is None: first_token_time = time.time()
                token_count += 1
            end_time = time.time()
            if first_token_time is None:
                result = (float('inf'), 0, float('inf'))
            else:
                ttft = first_token_time - start_time
                total_duration = end_time - start_time
                tps = (token_count - 1) / (total_duration - ttft) if (total_duration - ttft) > 0 else 0
                result = (ttft, tps, total_duration)
        else:
            response = ollama.generate(model=model_name, prompt=prompt, options=options, stream=False)
            result = response['response']
        queue.put(result)
    except Exception as e:
        queue.put(e)

def select_constraints_by_size(model_name: str):
    model_name = model_data.get('name') or model_data.get('model')
    size_b = get_model_size_in_billions(model_data['details'])
    print(f"åµæ¸¬åˆ°æ¨¡å‹ '{model_name}' çš„å¤§å°ç´„ç‚º {size_b:.2f}Bã€‚")
    chat_constraints = {"name": "å°å‹æ¨¡å‹ / èŠå¤©å ´æ™¯", "time_limit_s": 30.0, "ttft_limit_s": 2.0, "hallucination_threshold": 0.95, "num_predict": 128}
    summary_constraints = {"name": "ä¸­å‹æ¨¡å‹ / æ‘˜è¦å ´æ™¯", "time_limit_s": 120.0, "ttft_limit_s": 6.0, "num_predict": 512, "hallucination_threshold": 1.0}
    large_model_constraints = {"name": "å¤§å‹æ¨¡å‹ / è¤‡é›œä»»å‹™", "time_limit_s": 180.0, "ttft_limit_s": 15.0, "hallucination_threshold": 0.95, "num_predict": 256}
    if 0 < size_b < 10: return chat_constraints
    elif 10 <= size_b <= 18: return summary_constraints
    elif size_b > 18: return large_model_constraints
    else:
        print("ä½¿ç”¨é€šç”¨é è¨­ç´„æŸæ¢ä»¶ã€‚")
        return chat_constraints

def generate_html_report(results_data):
    report_filename = 'ollama_tuner_report.html'   
    try:
        if os.path.exists(report_filename):
            os.remove(report_filename)
            print(f"â“˜ å·²åˆªé™¤èˆŠçš„å ±å‘Šæª”æ¡ˆï¼š{report_filename}")

        env = Environment(loader=FileSystemLoader('.'))
        template = env.get_template('report_template.html')
        report_data = {'results': results_data, 'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
        html_content = template.render(report_data)
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(html_content)           
        print(f"\nâœ… HTML å ±å‘Šå·²æˆåŠŸç”Ÿæˆï¼š{report_filename}")
        
        try:
            webbrowser.open('file://' + os.path.abspath(report_filename))
            print("â“˜ æ­£åœ¨ä½ çš„é è¨­ç€è¦½å™¨ä¸­æ‰“é–‹å ±å‘Š...")
        except Exception as e:
            print(f"âŒ ç„¡æ³•è‡ªå‹•æ‰“é–‹ç€è¦½å™¨ï¼Œè«‹æ‰‹å‹•æ‰“é–‹æª”æ¡ˆã€‚éŒ¯èª¤: {e}")

    except Exception as e:
        print(f"\nâŒ ç”Ÿæˆ HTML å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

class OllamaAutoTuner:
    def __init__(self, model_name, constraints=None):
        self.model_name = model_name
        self.constraints = {
            "time_limit_s": 60.0, "ttft_limit_s": 5.0,
            "hallucination_threshold": 0.95, "num_predict": 256
        }
        if constraints: self.constraints.update(constraints)
        self.best_settings = {}
        self.current_process = None
        print("--- è‡ªå‹•èª¿æ ¡å™¨å·²åˆå§‹åŒ– ---")
        print("æ¨¡å‹:", self.model_name)
        print("ç´„æŸæ¢ä»¶:", self.constraints)

    def _safe_generate(self, prompt, settings, timeout):
        q = multiprocessing.Queue()
        p = multiprocessing.Process(target=ollama_generate_worker, args=(q, self.model_name, prompt, settings))
        try:
            self.current_process = p
            p.start()
            p.join(timeout=timeout)
            if p.is_alive():
                p.terminate()
                p.join()
                return None, "timeout"
            result = q.get_nowait()
            if isinstance(result, Exception):
                return None, str(result)
            return result, "success"
        except Empty:
            return None, "empty_queue"
        finally:
            self.current_process = None

    def _evaluate_hallucination(self, settings):
        scores = []
        is_incompatible = False
        for item in HALLUCINATION_EVAL_SET:
            prompt = f"è«‹åƒè€ƒä»¥ä¸‹è³‡è¨Šä¾†å›ç­”å•é¡Œã€‚\n\nä¸Šä¸‹æ–‡ï¼š{item['context']}\n\nå•é¡Œï¼š{item['question']}"
            process_settings = settings.copy(); process_settings['stream'] = False
            answer, status = self._safe_generate(prompt, process_settings, timeout=60.0)
            
            if status != "success" and "does not support generate" in status:
                is_incompatible = True
                break

            if status == "success" and isinstance(answer, str):
                if any(keyword in answer.lower() for keyword in item['ground_truth_keywords']):
                    scores.append(1.0)
                else: scores.append(0.0)
            else:
                print(f"  -> è­¦å‘Šï¼šå¹»è¦ºæª¢æ¸¬ item '{item['id']}' å¤±æ•— ({status})ï¼Œè¨ˆç‚º 0 åˆ†ã€‚")
                scores.append(0.0)
        
        if is_incompatible:
            return None          
        return sum(scores) / len(scores) if scores else 0.0

    def _measure_speed(self, prompt, settings):
        process_settings = settings.copy(); process_settings['stream'] = True
        timeout = self.constraints['time_limit_s'] + 10.0
        result, status = self._safe_generate(prompt, process_settings, timeout=timeout)
        if status == "success":
            return result
        else:
            print(f"  -> è­¦å‘Šï¼šé€Ÿåº¦æ¸¬è©¦å¤±æ•— ({status})ã€‚")
            return float('inf'), 0, float('inf')

    def tune_quality(self, temp_options=[1.0, 0.7, 0.5, 0.3], top_p_options=[1.0, 0.9, 0.95]):
        print("\n--- 1. é–‹å§‹é€²è¡Œå“è³ª (å¹»è¦º) èª¿æ ¡ ---")
        best_score = -1.0; best_quality_params = {}
        for temp, top_p in itertools.product(temp_options, top_p_options):
            print(f"æ¸¬è©¦çµ„åˆ: temperature={temp}, top_p={top_p}")
            score = self._evaluate_hallucination({"temperature": temp, "top_p": top_p, "seed": 42})
            
            if score is None:
                print(f"âŒ æ¨¡å‹ '{self.model_name}' ä¸æ”¯æ´ç”ŸæˆåŠŸèƒ½ï¼Œå·²è·³éã€‚")
                return "incompatible"

            print(f"å¹»è¦ºè©•åˆ†: {score:.2f}")
            if score > best_score:
                best_score = score
                best_quality_params = {"temperature": temp, "top_p": top_p}

        print(f"â“˜ æ‰¾åˆ°æœ€ä½³å“è³ªè¨­å®š: {best_quality_params} (æœ€é«˜åˆ†: {best_score:.2f})")
        self.best_settings.update(best_quality_params)
        self.best_settings['hallucination_score'] = round(best_score, 2)
        return "continue"

    def tune_context_window(self, ctx_options=[8192, 4096, 2048, 1024]):
        print("\n--- 2. é–‹å§‹é€²è¡Œä¸Šä¸‹æ–‡é•·åº¦ (num_ctx) èª¿æ ¡ ---")
        for ctx_size in ctx_options:
            print(f"æ­£åœ¨æ¸¬è©¦ num_ctx = {ctx_size}...")
            settings = self.best_settings.copy()
            settings.update({'num_ctx': ctx_size, 'num_predict': self.constraints['num_predict']})
            ttft, tps, duration = self._measure_speed(LONG_CONTEXT_PERFORMANCE_PROMPT, settings)
            print(f"  ç¸½æ™‚é–“: {duration:.2f}s (ä¸Šé™ {self.constraints['time_limit_s']:.1f}s), é¦–å­—å…ƒæ™‚é–“(TTFT): {ttft*1000:.0f}ms (ä¸Šé™ {self.constraints['ttft_limit_s']*1000:.0f}ms)")
            if duration < self.constraints['time_limit_s'] and ttft < self.constraints['ttft_limit_s']:
                print(f"âœ… æ‰¾åˆ°å¯æ¥å—çš„ num_ctx è¨­å®š: {ctx_size}")
                self.best_settings['num_ctx'] = ctx_size
                return True
        print(f"âŒ æœªèƒ½æ‰¾åˆ°æ»¿è¶³æ™‚é–“è¦æ±‚çš„ num_ctx è¨­å®šã€‚å°‡ä½¿ç”¨æœ€å°å€¼ {ctx_options[-1]}ã€‚")
        self.best_settings['num_ctx'] = ctx_options[-1]
        return False

    def tune_gpu_layers(self):
        print("\n--- 3. é–‹å§‹é€²è¡Œ GPU åŠ é€Ÿ (num_gpu) èª¿æ ¡ (é«˜æ•ˆæ¨¡å¼) ---")
        low, high = 0, 101; best_working_gpu = 0
        while low < high:
            mid = low + (high - low) // 2
            if mid == low: mid = high
            if mid == high and mid == best_working_gpu: break
            print(f"æ­£åœ¨äºŒåˆ†æœå°‹æ¸¬è©¦ num_gpu = {mid}...")
            settings = self.best_settings.copy()
            settings.update({'num_gpu': mid, 'num_predict': self.constraints['num_predict']})
            ttft, tps, duration = self._measure_speed(LONG_CONTEXT_PERFORMANCE_PROMPT, settings)
            print(f"  ç¸½æ™‚é–“: {duration:.2f}s, TTFT: {ttft*1000:.0f}ms, é€Ÿåº¦: {tps:.2f} tokens/s")
            if duration < self.constraints['time_limit_s'] and ttft < self.constraints['ttft_limit_s']:
                print(f"  -> æˆåŠŸã€‚å˜—è©¦åœ¨ [{mid}, {high-1}] ç¯„åœå…§å°‹æ‰¾æ›´é«˜å€¼ã€‚")
                best_working_gpu = mid; low = mid
            else:
                print(f"  -> å¤±æ•—ã€‚åœ¨ [{low}, {mid-1}] ç¯„åœå…§å°‹æ‰¾ã€‚")
                high = mid - 1
        print(f"\näºŒåˆ†æœå°‹å®Œæˆï¼Œæ‰¾åˆ°çš„æœ€ä½³å€™é¸å€¼ç‚º {best_working_gpu}ã€‚é€²è¡Œæœ€çµ‚ç¢ºèª...")
        self.best_settings['num_gpu'] = best_working_gpu
        settings = self.best_settings.copy(); settings['num_predict'] = self.constraints['num_predict']
        ttft, tps, duration = self._measure_speed(LONG_CONTEXT_PERFORMANCE_PROMPT, settings)
        final_performance = {'ttft': ttft, 'tps': tps, 'duration': duration}
        if duration < self.constraints['time_limit_s'] and ttft < self.constraints['ttft_limit_s']:
             print(f"âœ… æœ€çµ‚ç¢ºèªæˆåŠŸï¼æœ€ä½³ num_gpu è¨­å®šç‚º: {best_working_gpu}")
             return True, final_performance
        else:
             print(f"âŒ æœ€çµ‚ç¢ºèªå¤±æ•—ã€‚å¯èƒ½ç³»çµ±è² è¼‰ä¸ç©©å®šã€‚å°‡ä½¿ç”¨ CPU (num_gpu=0)ã€‚")
             self.best_settings['num_gpu'] = 0; settings['num_gpu'] = 0
             ttft, tps, duration = self._measure_speed(LONG_CONTEXT_PERFORMANCE_PROMPT, settings)
             final_performance = {'ttft': ttft, 'tps': tps, 'duration': duration}
             return False, final_performance

    def run(self):
        try:
            quality_status = self.tune_quality()
            if quality_status == "incompatible":
                return "incompatible"
            self.tune_context_window()
            success, final_performance = self.tune_gpu_layers()
            
            print("\n--- æœ€çµ‚æœ€ä½³åŒ–è¨­å®š ---")
            final_settings = self.best_settings.copy()
            final_settings['num_predict'] = self.constraints['num_predict']
            print(pd.Series(final_settings).to_string())
            return {"model_name": self.model_name, "optimal_settings": final_settings,
                    "constraints": self.constraints, "final_performance": final_performance}
        except KeyboardInterrupt:
            print("\n\nğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–·è¨Šè™Ÿ (Ctrl+C)ï¼Œæ­£åœ¨æ¸…ç†ä¸¦çµæŸ...")
            if self.current_process and self.current_process.is_alive():
                print(f"  -> æ­£åœ¨çµ‚æ­¢æ­£åœ¨é‹è¡Œçš„å­é€²ç¨‹ (PID: {self.current_process.pid})...")
                self.current_process.terminate()
                self.current_process.join()
                print("  -> å­é€²ç¨‹å·²æ¸…ç†ã€‚")
            print("ç¨‹å¼å·²å®‰å…¨åœæ­¢ã€‚")
            sys.exit(0)

if __name__ == "__main__":
    print("æ­£åœ¨å¾æœ¬åœ° Ollama åµæ¸¬å·²å®‰è£çš„æ¨¡å‹...")
    MODELS_TO_TUNE = get_local_ollama_models()
    
    if not MODELS_TO_TUNE:
        print("\næœªæ‰¾åˆ°ä»»ä½•æœ¬åœ° Ollama æ¨¡å‹ï¼Œæˆ–ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ã€‚è…³æœ¬å·²åœæ­¢ã€‚")
    else:
        detected_model_names = [m.get('name') or m.get('model') for m in MODELS_TO_TUNE]
        print(f"åµæ¸¬æˆåŠŸï¼å°‡å°ä»¥ä¸‹æ¨¡å‹é€²è¡Œèª¿æ ¡ï¼š{detected_model_names}")
        
        all_results = []
        for model_data in MODELS_TO_TUNE:
            model_name = model_data.get('name') or model_data.get('model')
            if not model_name:
                print(f"è­¦å‘Šï¼šè·³éä¸€å€‹æ²’æœ‰åç¨±çš„æ¨¡å‹æ¢ç›®ï¼š{model_data}")
                continue
            print(f"\n{'='*20} é–‹å§‹ç‚º '{model_name}' é€²è¡Œèª¿æ ¡ {'='*20}")

            selected_constraints = select_constraints_by_size(model_data)
            
            print(f"è‡ªå‹•é¸æ“‡äº† '{selected_constraints['name']}' çš„ç´„æŸæ¢ä»¶ã€‚")
            tuner_constraints = selected_constraints.copy()
            del tuner_constraints['name']
            tuner = OllamaAutoTuner(model_name=model_name, constraints=tuner_constraints)
            result = tuner.run()
            
            if result and result != "incompatible":
                all_results.append(result)
            print(f"\n{'='*20} '{model_name}' èª¿æ ¡å®Œæˆ {'='*20}\n")
            
        if all_results:
            generate_html_report(all_results)
        else:
            print("æ²’æœ‰ä»»ä½•ç›¸å®¹çš„æ¨¡å‹æˆåŠŸå®Œæˆèª¿æ ¡ï¼Œç„¡æ³•ç”Ÿæˆå ±å‘Šã€‚")
