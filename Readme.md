# ğŸš€ å¢å¼·çš„ Ollama Auto-Tuner - æœ¬åœ° LLM æ™ºèƒ½å„ªåŒ–èˆ‡ç›£æ§å·¥å…·

å¢å¼·çš„ Ollama Auto-Tuner æ˜¯ä¸€å€‹ä½¿ç”¨ Python é–‹ç™¼çš„å·¥å…·ï¼Œæ—¨åœ¨è‡ªå‹•åŒ–åœ°ç‚ºæ‚¨æœ¬åœ°é‹è¡Œçš„ Ollama å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰å°‹æ‰¾æœ€ä½³çš„æ€§èƒ½èˆ‡å“è³ªè¨­å®šã€‚æœ¬å·¥å…·æ¡ç”¨æ¨¡çµ„åŒ–æ¶æ§‹ï¼Œæ•´åˆäº†å¤šé …å…ˆé€²æŠ€è¡“ï¼Œä¸¦æä¾›äº†ä¸€å€‹äº’å‹•å¼çš„ Web UI ä¾†å³æ™‚ç›£æ§èª¿æ ¡éç¨‹ã€‚

[English Version](#-enhanced-ollama-auto-tuner---intelligent-optimization--monitoring-tool-for-local-llms)

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

- **äº’å‹•å¼ Web UI**ï¼šé€éç¶²é ä»‹é¢å³æ™‚ç›£æ§ç³»çµ±è³‡æºã€èª¿æ ¡é€²åº¦ã€æ—¥èªŒå’Œçµæœï¼Œä¸¦å¯é ç¨‹å•Ÿå‹•/åœæ­¢èª¿æ ¡ä»»å‹™ã€‚
- **æ™ºèƒ½å„ªåŒ–ç®—æ³•**ï¼šä½¿ç”¨è²è‘‰æ–¯å„ªåŒ–ï¼Œç”¨æ›´å°‘çš„æ¸¬è©¦æ¬¡æ•¸æ‰¾åˆ°æ›´å„ªçš„åƒæ•¸çµ„åˆã€‚
- **æ—©æœŸåœæ­¢æ©Ÿåˆ¶**ï¼šåœ¨å„ªåŒ–åœæ»¯æ™‚è‡ªå‹•çµæŸï¼Œç¯€çœæ™‚é–“å’Œè³‡æºã€‚
- **å…¨æ–¹ä½æ€§èƒ½å„ªåŒ–**ï¼šåŒ…å« GPU/CPU è¨˜æ†¶é«”ç›£æ§ã€æ™ºèƒ½ç·©å­˜ï¼Œç¢ºä¿æ¸¬è©¦æµç¨‹ç©©å®šé«˜æ•ˆã€‚
- **å¢å¼·è©•ä¼°ç³»çµ±**ï¼šå¾å›ç­”ç›¸é—œæ€§ã€é‚è¼¯ä¸€è‡´æ€§ã€äº‹å¯¦æº–ç¢ºæ€§ç­‰å¤šå€‹ç¶­åº¦é€²è¡Œç¶œåˆå“è³ªè©•åˆ†ã€‚
- **è©³ç´°è¦–è¦ºåŒ–å ±å‘Š**ï¼šæ¸¬è©¦å®Œæˆå¾Œï¼Œå¯ç”ŸæˆåŒ…å«å‹•æ…‹åœ–è¡¨çš„äº’å‹•å¼ HTML å ±å‘Šã€‚

### ğŸ—ï¸ æ¨¡çµ„åŒ–æ¶æ§‹
```
src/
â”œâ”€â”€ core/               # æ ¸å¿ƒåŠŸèƒ½æ¨¡çµ„
â”‚   â”œâ”€â”€ enhanced_tuner.py
â”‚   â””â”€â”€ enhanced_evaluator.py
â”œâ”€â”€ models/             # å„ªåŒ–ç®—æ³•æ¨¡çµ„
â”‚   â””â”€â”€ bayesian_optimizer.py
â”œâ”€â”€ ui/                 # Web UI æ¨¡çµ„
â”‚   â”œâ”€â”€ web_interface.py
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html
â””â”€â”€ utils/              # å·¥å…·æ¨¡çµ„
    â”œâ”€â”€ memory_monitor.py
    â”œâ”€â”€ cache_manager.py
    â””â”€â”€ ollama_utils.py
```

## ğŸ› ï¸ å®‰è£æŒ‡å—

1.  **ç¢ºä¿ Ollama å·²å®‰è£ä¸¦é‹è¡Œ**
    è«‹æ ¹æ“š [Ollama å®˜æ–¹ç¶²ç«™](https://ollama.com/) çš„æŒ‡å¼•å®‰è£ Ollamaï¼Œä¸¦ç¢ºä¿å…¶æœå‹™æ­£åœ¨èƒŒæ™¯é‹è¡Œã€‚

2.  **æ‹‰å–æ‚¨æƒ³æ¸¬è©¦çš„æ¨¡å‹**
    ```bash
    ollama pull llama3:8b
    ```

3.  **å…‹éš†æœ¬å°ˆæ¡ˆ** (å¦‚æœæ‚¨å°šæœªæ“ä½œ)
    ```bash
    git clone https://github.com/your-username/ollama-auto-tuner.git
    cd ollama-auto-tuner
    ```

4.  **å®‰è£ Python ä¾è³´é …**
    å»ºè­°åœ¨è™›æ“¬ç’°å¢ƒä¸­é€²è¡Œå®‰è£ï¼š
    ```bash
    python -m venv venv
    source venv/bin/activate  # åœ¨ Windows ä¸Šä½¿ç”¨ `venv\Scripts\activate`
    pip install -r requirements.txt
    ```

## ğŸš€ å¦‚ä½•ä½¿ç”¨

æœ¬å·¥å…·æä¾›å…©ç¨®ä½¿ç”¨æ¨¡å¼ï¼šäº’å‹•å¼ Web UI (æ¨è–¦) å’Œå‚³çµ±çš„å‘½ä»¤åˆ—ä»‹é¢ (CLI)ã€‚

### æ–¹æ³•ä¸€ï¼šäº’å‹•å¼ Web UI (æ¨è–¦)

é€™æ˜¯æœ€æ–¹ä¾¿çš„ä½¿ç”¨æ–¹å¼ï¼Œå¯ä»¥å³æ™‚ç›£æ§æ‰€æœ‰ç‹€æ…‹ã€‚

1.  **å•Ÿå‹• Web æœå‹™**ï¼š
    ```bash
    python web_ollama_autotuner.py
    ```

2.  **æ‰“é–‹ç€è¦½å™¨**ï¼š
    åœ¨ç€è¦½å™¨ä¸­è¨ªå• `http://127.0.0.1:5000` (æˆ–æ—¥èªŒä¸­é¡¯ç¤ºçš„æ‚¨çš„ IP åœ°å€)ã€‚

3.  **é–‹å§‹èª¿æ ¡**ï¼š
    åœ¨ç¶²é ä¸Šï¼Œæ‚¨å¯ä»¥å¾ä¸‹æ‹‰é¸å–®ä¸­é¸æ“‡ç‰¹å®šæ¨¡å‹æˆ–æ‰€æœ‰æ¨¡å‹ï¼Œç„¶å¾Œé»æ“Šã€Œé–‹å§‹èª¿æ ¡ã€ã€‚èª¿æ ¡éç¨‹ä¸­çš„æ—¥èªŒã€ç³»çµ±è³‡æºå’Œå³æ™‚çµæœéƒ½æœƒé¡¯ç¤ºåœ¨é é¢ä¸Šã€‚

4.  **æŸ¥çœ‹å ±å‘Š**ï¼š
    ç•¶æ‰€æœ‰ä»»å‹™å®Œæˆå¾Œï¼Œã€Œç”¢ç”Ÿå®Œæ•´å ±å‘Šã€æŒ‰éˆ•æœƒè¢«å•Ÿç”¨ã€‚é»æ“Šå®ƒï¼Œç¨‹å¼æœƒè‡ªå‹•ç‚ºæ‚¨ç”Ÿæˆè©³ç´°çš„ HTML å ±å‘Šä¸¦åœ¨æ–°åˆ†é ä¸­é–‹å•Ÿã€‚

### æ–¹æ³•äºŒï¼šå‘½ä»¤åˆ—ä»‹é¢ (CLI)

é©åˆè‡ªå‹•åŒ–æˆ–ä¸ä¾è³´åœ–å½¢ä»‹é¢çš„å ´æ™¯ã€‚

1.  **åŸºæœ¬ä½¿ç”¨** (æ¸¬è©¦æ‰€æœ‰æ¨¡å‹):
    ```bash
    python enhanced_ollama_autotuner.py
    ```

2.  **é€²éšé¸é …**:
    ```bash
    # æŒ‡å®šå–®ä¸€æ¨¡å‹é€²è¡Œæ¸¬è©¦
    python enhanced_ollama_autotuner.py --model llama3:8b

    # è‡ªå®šç¾©æ™‚é–“å’Œ TTFT é™åˆ¶
    python enhanced_ollama_autotuner.py --time-limit 120 --ttft-limit 5

    # å•Ÿç”¨æ›´è©³ç´°çš„æ—¥èªŒè¼¸å‡º
    python enhanced_ollama_autotuner.py --verbose
    ```

## ğŸ“‹ ä¾è³´å¥—ä»¶

- **æ ¸å¿ƒä¾è³´**: `ollama`, `pandas`
- **å ±å‘Šç”Ÿæˆ**: `Jinja2`
- **ç³»çµ±ç›£æ§**: `psutil`, `GPUtil`
- **æ©Ÿå™¨å­¸ç¿’èˆ‡å„ªåŒ–**: `scikit-learn`, `scipy`, `numpy`
- **Web UI (å¯é¸)**: `Flask`, `Flask-SocketIO`
- **åœ–è¡¨ç”Ÿæˆ (å¯é¸)**: `matplotlib`, `seaborn`

## ğŸ“ æ›´æ–°æ—¥èªŒ

### v3.0.0 (æœ€æ–°)
- âœ¨ **æ–°å¢åŠŸèƒ½**ï¼šå»ºç«‹äº†åŠŸèƒ½å®Œå–„çš„äº’å‹•å¼ Web UIï¼Œç”¨æ–¼å³æ™‚ç›£æ§å’Œæ§åˆ¶ã€‚
- ğŸ› **éŒ¯èª¤ä¿®å¾©**ï¼šä¿®å¾©äº†å‘½ä»¤åˆ—åƒæ•¸ç„¡æ³•æ­£å¸¸å·¥ä½œçš„å•é¡Œã€‚
- ğŸ› **éŒ¯èª¤ä¿®å¾©**ï¼šä¿®æ­£äº†å›  `ollama` å‡½å¼åº« API è®Šæ›´å°è‡´ç„¡æ³•æ­£ç¢ºè®€å–æ¨¡å‹åˆ—è¡¨çš„å•é¡Œã€‚
- ğŸ› **éŒ¯èª¤ä¿®å¾©**ï¼šä¿®å¾©äº† Web UI ä¸­çš„å¤šå€‹ JavaScript éŒ¯èª¤å’Œæ™‚åºå•é¡Œã€‚
- ğŸ¨ **æ¨£å¼æ”¹é€²**ï¼šçµ±ä¸€äº† HTML å ±å‘Šä¸­çš„èƒŒæ™¯æ¨£å¼ï¼Œä½¿å…¶èƒ½æ­£ç¢ºéŸ¿æ‡‰ä¸»é¡Œåˆ‡æ›ã€‚
- ğŸ—ï¸ **æ¶æ§‹é‡æ§‹**ï¼šé‡æ§‹äº† Web UI çš„å•Ÿå‹•å’Œè³‡æ–™è™•ç†æµç¨‹ï¼Œä½¿å…¶æ›´ç©©å®šã€‚

### v2.0.0
- âœ¨ æ–°å¢è²è‘‰æ–¯å„ªåŒ–ç®—æ³•
- ğŸ§  å¯¦æ™‚è¨˜æ†¶é«”ç›£æ§
- ğŸ’¾ æ™ºèƒ½ç·©å­˜ç³»çµ±
- ğŸ“Š å¢å¼·è©•ä¼°æŒ‡æ¨™
- ğŸ¨ äº’å‹•å¼å ±å‘Š
- ğŸ—ï¸ æ¨¡çµ„åŒ–æ¶æ§‹é‡æ§‹

---

# ğŸš€ Enhanced Ollama Auto-Tuner - Intelligent Optimization & Monitoring Tool for Local LLMs

The Enhanced Ollama Auto-Tuner is a Python tool designed to automatically find optimal performance and quality settings for your locally running Ollama Large Language Models (LLMs). This tool adopts a modular architecture, integrates multiple advanced technologies, and provides an interactive Web UI to monitor the tuning process in real-time.

## âœ¨ Core Features

- **Interactive Web UI**: Monitor system resources, tuning progress, logs, and results in real-time via a web interface, with remote control to start/stop tuning tasks.
- **Intelligent Optimization**: Uses Bayesian Optimization to find better parameter combinations with fewer tests.
- **Early Stopping**: Automatically ends the process when optimization stalls, saving time and resources.
- **Comprehensive Performance Optimization**: Includes GPU/CPU memory monitoring and smart caching to ensure a stable and efficient testing process.
- **Enhanced Evaluation System**: Provides a comprehensive quality score from multiple dimensions like relevance, logical consistency, and factual accuracy.
- **Detailed Visual Reports**: Generates interactive HTML reports with dynamic charts upon completion.

## ğŸš€ How to Use

This tool offers two modes of operation: the interactive Web UI (recommended) and the traditional Command-Line Interface (CLI).

### Method 1: Interactive Web UI (Recommended)

1.  **Launch the Web Service**:
    ```bash
    python web_ollama_autotuner.py
    ```

2.  **Open Your Browser**:
    Navigate to `http://127.0.0.1:5000`.

3.  **Start Tuning**:
    On the web page, you can select a specific model or all models from the dropdown and click "Start Tuning".

4.  **View Report**:
    Once all tasks are complete, the "Generate Full Report" button will be enabled. Clicking it will generate a detailed HTML report and open it in a new tab.

### Method 2: Command-Line Interface (CLI)

1.  **Basic Usage** (test all models):
    ```bash
    python enhanced_ollama_autotuner.py
    ```

2.  **Advanced Options**:
    ```bash
    # Specify a single model to test
    python enhanced_ollama_autotuner.py --model llama3:8b
    ```