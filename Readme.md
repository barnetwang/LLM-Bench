# ğŸš€ Ollama Auto-Tuner - Automated Optimization Tool for Local LLMs

Ollama Auto-Tuner is a powerful tool developed in Python, designed to automatically find the optimal performance and quality settings for your locally running Ollama Large Language Models (LLMs).

[ä¸­æ–‡ç‰ˆèªªæ˜](#-ollama-auto-tuner---æœ¬åœ°-llm-è‡ªå‹•åŒ–æœ€ä½³åŒ–å·¥å…·)

Have you ever been puzzled by questions like:
*   How many layers of my model should I offload to the GPU (`num_gpu`) to run efficiently without sacrificing too much speed?
*   What should I set `temperature` and `top_p` to, for more accurate, less hallucinatory responses?
*   Which of my models runs fastest on my hardware while also providing the best quality?

This tool automatically answers these questions for you and generates a clear, professional HTML report.

## âœ¨ Core Features

*   **Fully Automated Workflow**: Automatically detects all your locally installed Ollama models and tests them one by one.
*   **Intelligent Constraint Selection**: Automatically chooses a reasonable set of performance constraints based on the model's size (e.g., 7B, 20B).
*   **Quality-First Tuning**: Finds the best `temperature` and `top_p` combination for a model by running it against a carefully designed "hallucination evaluation" benchmark to maximize response accuracy.
*   **Efficient Performance Exploration**: Uses a binary search algorithm to efficiently determine the maximum GPU layers (`num_gpu`) and context window (`num_ctx`) that can be used within your time limits.
*   **Robust Execution Engine**:
    *   Uses multiprocessing to ensure that a single stalled or timed-out test won't crash the entire process.
    *   Supports safe interruption with `Ctrl+C`, allowing you to stop the tests at any time and clean up background processes.
    *   Automatically skips incompatible models (e.g., embedding models).
*   **Professional Report Generation**:
    *   Automatically generates a beautiful HTML report (`ollama_tuner_report.html`) after all tests are complete.
    *   The report includes a summary comparison table, detailed performance metrics (TTFT, TPS), optimized settings, and quality scores for each model.
    *   Automatically opens the report in your default web browser upon completion.

## ğŸ“Š Sample Report

*(Hint: You can take a screenshot of your report and replace this link)*

## ğŸ› ï¸ Installation Guide

1.  **Ensure Ollama is Installed and Running**
    Please follow the instructions on the [Ollama Official Website](https://ollama.com/) to install Ollama and make sure its service is running in the background.

2.  **Pull the Models You Want to Test**
    In your terminal, use the `ollama pull` command to download the models you are interested in. For example:
    ```bash
    ollama pull llama3:8b
    ollama pull qwen:14b
    ```

3.  **Clone This Project**
    ```bash
    git clone https://github.com/your-username/ollama-auto-tuner.git
    cd ollama-auto-tuner
    ```

4.  **Install Python Dependencies**
    It is recommended to install in a virtual environment.
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    pip install -r requirements.txt
    ```

## ğŸš€ How to Use

Running the Auto-Tuner is simple. Just execute the main script:

```bash
python ollama_autotuner.py
```

The script will automatically perform the following steps:
1.  Connect to your local Ollama service and detect all installed models.
2.  Select appropriate testing criteria for each model.
3.  Perform quality and performance tuning for each model sequentially.
4.  Print a detailed log of the testing process and final settings in the terminal.
5.  After all tests are complete, generate `ollama_tuner_report.html` and open it in your browser.

## ğŸ§¬ How to Extend and Customize

You can easily customize this tool to meet your specific needs:

*   **Adjust Tuning Parameters**: In the `tune_quality` and `tune_context_window` methods of `ollama_autotuner.py`, you can modify the lists of options for `temperature`, `top_p`, and `num_ctx`.

*   **Enhance Hallucination Tests**: Edit the `evaluation_dataset.py` file to add more complex and challenging test cases to better evaluate model quality.

*   **Modify Constraints**: In the `select_constraints_by_size` function in `ollama_autotuner.py`, you can change the performance constraints (like `time_limit_s`, `ttft_limit_s`) for different model sizes.

*   **Beautify the Report**: Directly edit the `report_template.html` file. You can freely change the style, layout, or add charts (e.g., using [Chart.js](https://www.chartjs.org/)).

## ğŸ™ Acknowledgements

This project was born from an idea and gradually refined through numerous iterations and debugging sessions with an AI assistant. Thanks to all open-source community contributors, especially the Ollama team.

---

# ğŸš€ Ollama Auto-Tuner - æœ¬åœ° LLM è‡ªå‹•åŒ–æœ€ä½³åŒ–å·¥å…·

Ollama Auto-Tuner æ˜¯ä¸€å€‹ä½¿ç”¨ Python é–‹ç™¼çš„å¼·å¤§å·¥å…·ï¼Œæ—¨åœ¨è‡ªå‹•åŒ–åœ°ç‚ºæ‚¨æœ¬åœ°é‹è¡Œçš„ Ollama å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰å°‹æ‰¾æœ€ä½³çš„æ€§èƒ½èˆ‡å“è³ªè¨­å®šã€‚

æ‚¨æ˜¯å¦æ›¾å›°æ“¾æ–¼ä»¥ä¸‹å•é¡Œï¼š
*   æˆ‘çš„æ¨¡å‹æ‡‰è©²åˆ†é…å¤šå°‘å±¤åˆ° GPU (`num_gpu`) æ‰èƒ½åœ¨ä¸çŠ§ç‰²å¤ªå¤šé€Ÿåº¦çš„æƒ…æ³ä¸‹é‹è¡Œï¼Ÿ
*   `temperature` å’Œ `top_p` æ‡‰è©²è¨­ç‚ºå¤šå°‘ï¼Œæ‰èƒ½è®“æ¨¡å‹å›ç­”å¾—æ›´æº–ç¢ºï¼Œæ¸›å°‘å¹»è¦ºï¼Ÿ
*   å“ªå€‹æ¨¡å‹åœ¨æˆ‘çš„ç¡¬é«”ä¸Šè·‘å¾—æœ€å¿«ï¼ŒåŒæ™‚å“è³ªä¹Ÿæœ€å¥½ï¼Ÿ

é€™å€‹å·¥å…·å°‡ç‚ºæ‚¨è‡ªå‹•è§£ç­”é€™äº›å•é¡Œï¼Œä¸¦ç”Ÿæˆä¸€ä»½æ¸…æ™°ã€å°ˆæ¥­çš„ HTML å ±å‘Šã€‚

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

*   **å…¨è‡ªå‹•åŒ–æµç¨‹**ï¼šè‡ªå‹•åµæ¸¬æ‚¨æœ¬åœ°å·²å®‰è£çš„æ‰€æœ‰ Ollama æ¨¡å‹ï¼Œä¸¦é€ä¸€é€²è¡Œæ¸¬è©¦ã€‚
*   **æ™ºæ…§ç´„æŸé¸æ“‡**ï¼šæ ¹æ“šæ¨¡å‹çš„å¤§å°ï¼ˆä¾‹å¦‚ 7B, 20Bï¼‰è‡ªå‹•é¸æ“‡ä¸€å¥—åˆç†çš„æ€§èƒ½æ¸¬è©¦ç´„æŸæ¢ä»¶ã€‚
*   **å“è³ªå„ªå…ˆèª¿æ ¡**ï¼šé€éä¸€å¥—ç²¾å¿ƒè¨­è¨ˆçš„ã€Œå¹»è¦ºè©•ä¼°ã€åŸºæº–æ¸¬è©¦ï¼Œç‚ºæ¨¡å‹æ‰¾åˆ°æœ€ä½³çš„ `temperature` å’Œ `top_p` çµ„åˆï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜å›ç­”çš„æº–ç¢ºæ€§ã€‚
*   **é«˜æ•ˆæ€§èƒ½æ¢ç´¢**ï¼šä½¿ç”¨äºŒåˆ†æœå°‹æ³•é«˜æ•ˆåœ°ç¢ºå®šåœ¨æ»¿è¶³æ™‚é–“é™åˆ¶çš„å‰æä¸‹ï¼Œå¯ä»¥åˆ†é…åˆ° GPU çš„æœ€å¤§å±¤æ•¸ (`num_gpu`) å’Œå¯ç”¨çš„æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ (`num_ctx`)ã€‚
*   **å¥å£¯çš„åŸ·è¡Œæ©Ÿåˆ¶**ï¼š
    *   ä½¿ç”¨å¤šé€²ç¨‹ï¼ˆmultiprocessingï¼‰ç¢ºä¿ä»»ä½•å–®ä¸€æ¸¬è©¦çš„å¡æ­»æˆ–è¶…æ™‚éƒ½ä¸æœƒå½±éŸ¿æ•´å€‹æµç¨‹ã€‚
    *   æ”¯æ´ `Ctrl+C` å®‰å…¨ä¸­æ–·ï¼Œå¯ä»¥éš¨æ™‚åœæ­¢æ¸¬è©¦ä¸¦æ¸…ç†èƒŒæ™¯é€²ç¨‹ã€‚
    *   è‡ªå‹•è·³éä¸ç›¸å®¹çš„æ¨¡å‹ï¼ˆå¦‚ Embedding æ¨¡å‹ï¼‰ã€‚
*   **å°ˆæ¥­å ±å‘Šç”Ÿæˆ**ï¼š
    *   åœ¨æ‰€æœ‰æ¸¬è©¦çµæŸå¾Œï¼Œè‡ªå‹•ç”Ÿæˆä¸€ä»½ç²¾ç¾çš„ HTML å ±å‘Š (`ollama_tuner_report.html`)ã€‚
    *   å ±å‘ŠåŒ…å«ç¸½çµå°æ¯”è¡¨æ ¼ã€æ¯å€‹æ¨¡å‹çš„è©³ç´°æ€§èƒ½æŒ‡æ¨™ï¼ˆTTFT, TPSï¼‰ã€æœ€ä½³åŒ–è¨­å®šå’Œå“è³ªè©•åˆ†ã€‚
    *   å®Œæˆå¾Œè‡ªå‹•åœ¨æ‚¨çš„é è¨­ç€è¦½å™¨ä¸­æ‰“é–‹å ±å‘Šã€‚

## ğŸ“Š å ±å‘Šç¯„ä¾‹

*(æç¤º: æ‚¨å¯ä»¥å°‡æ‚¨çš„å ±å‘Šæˆªåœ–ä¸¦æ›¿æ›æ­¤è™•çš„é€£çµ)*

## ğŸ› ï¸ å®‰è£æŒ‡å—

1.  **ç¢ºä¿ Ollama å·²å®‰è£ä¸¦é‹è¡Œ**
    è«‹æ ¹æ“š [Ollama å®˜æ–¹ç¶²ç«™](https://ollama.com/) çš„æŒ‡å¼•å®‰è£ Ollamaï¼Œä¸¦ç¢ºä¿å…¶æœå‹™æ­£åœ¨èƒŒæ™¯é‹è¡Œã€‚

2.  **æ‹‰å–æ‚¨æƒ³æ¸¬è©¦çš„æ¨¡å‹**
    åœ¨çµ‚ç«¯æ©Ÿä¸­ï¼Œä½¿ç”¨ `ollama pull` å‘½ä»¤ä¸‹è¼‰æ‚¨æ„Ÿèˆˆè¶£çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼š
    ```bash
    ollama pull llama3:8b
    ollama pull qwen:14b
    ```

3.  **å…‹éš†æœ¬å°ˆæ¡ˆ**
    ```bash
    git clone https://github.com/your-username/ollama-auto-tuner.git
    cd ollama-auto-tuner
    ```

4.  **å®‰è£ Python ä¾è³´é …**
    å»ºè­°åœ¨è™›æ“¬ç’°å¢ƒä¸­é€²è¡Œå®‰è£ã€‚
    ```bash
    python -m venv venv
    source venv/bin/activate  # åœ¨ Windows ä¸Šä½¿ç”¨ `venv\Scripts\activate`
    pip install -r requirements.txt
    ```

## ğŸš€ å¦‚ä½•ä½¿ç”¨

é‹è¡Œ Auto-Tuner éå¸¸ç°¡å–®ï¼Œåªéœ€è¦åŸ·è¡Œä¸»è…³æœ¬å³å¯ï¼š

```bash
python ollama_autotuner.py
```

ç¨‹å¼å°‡æœƒè‡ªå‹•åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š
1.  é€£æ¥åˆ°æ‚¨çš„æœ¬åœ° Ollama æœå‹™ä¸¦åµæ¸¬æ‰€æœ‰å·²å®‰è£çš„æ¨¡å‹ã€‚
2.  ç‚ºæ¯å€‹æ¨¡å‹é¸æ“‡åˆé©çš„æ¸¬è©¦æ¨™æº–ã€‚
3.  é€ä¸€å°æ¯å€‹æ¨¡å‹é€²è¡Œå“è³ªå’Œæ€§èƒ½èª¿æ ¡ã€‚
4.  åœ¨çµ‚ç«¯æ©Ÿä¸­æ‰“å°è©³ç´°çš„æ¸¬è©¦éç¨‹å’Œæœ€çµ‚è¨­å®šã€‚
5.  æ‰€æœ‰æ¸¬è©¦å®Œæˆå¾Œï¼Œç”Ÿæˆ `ollama_tuner_report.html` ä¸¦è‡ªå‹•åœ¨ç€è¦½å™¨ä¸­æ‰“é–‹ã€‚

## ğŸ§¬ å¦‚ä½•æ“´å±•èˆ‡å®¢è£½åŒ–

æ‚¨å¯ä»¥è¼•é¬†åœ°å®¢è£½åŒ–æ­¤å·¥å…·ä»¥æ»¿è¶³æ‚¨çš„ç‰¹å®šéœ€æ±‚ï¼š

*   **èª¿æ•´æ¸¬è©¦åƒæ•¸**ï¼šåœ¨ `ollama_autotuner.py` çš„ `tune_quality` å’Œ `tune_context_window` æ–¹æ³•ä¸­ï¼Œæ‚¨å¯ä»¥ä¿®æ”¹è¦æ¸¬è©¦çš„ `temperature`, `top_p` å’Œ `num_ctx` çš„é¸é …åˆ—è¡¨ã€‚

*   **å¢å¼·å¹»è¦ºæ¸¬è©¦**ï¼šç·¨è¼¯ `evaluation_dataset.py` æª”æ¡ˆï¼ŒåŠ å…¥æ›´å¤šã€æ›´åˆé‘½çš„æ¸¬è©¦é¡Œç›®ï¼Œä»¥æ›´å¥½åœ°è©•ä¼°æ¨¡å‹çš„å“è³ªã€‚

*   **ä¿®æ”¹ç´„æŸæ¢ä»¶**ï¼šåœ¨ `ollama_autotuner.py` çš„ `select_constraints_by_size` å‡½å¼ä¸­ï¼Œæ‚¨å¯ä»¥ä¿®æ”¹ä¸åŒå¤§å°æ¨¡å‹çš„æ€§èƒ½ç´„æŸæ¢ä»¶ï¼ˆå¦‚ `time_limit_s`, `ttft_limit_s`ï¼‰ã€‚

*   **ç¾åŒ–å ±å‘Š**ï¼šç›´æ¥ç·¨è¼¯ `report_template.html` æª”æ¡ˆï¼Œæ‚¨å¯ä»¥è‡ªç”±åœ°ä¿®æ”¹å ±å‘Šçš„æ¨£å¼ã€ä½ˆå±€æˆ–æ–°å¢åœ–è¡¨ï¼ˆä¾‹å¦‚ä½¿ç”¨ [Chart.js](https://www.chartjs.org/)ï¼‰ã€‚

## âš–ï¸ æˆæ¬Šèˆ‡æ„Ÿè¬ / License & Acknowledgements

æ­¤å°ˆæ¡ˆç‚ºé–‹æºé …ç›®ï¼Œä½¿ç”¨äº†å¤šå€‹ç¬¬ä¸‰æ–¹å‡½å¼åº«ã€‚
This project is open-source and uses various third-party libraries.
è«‹ä»”ç´°é–±è®€å…¶æˆæ¬Šæ¢æ¬¾ï¼Œåœ¨å•†æ¥­ç”¨é€”å‰å‹™å¿…å¯©è¦–æ¸…æ¥šã€‚
Please review their licenses carefully before using this code for commercial purposes.